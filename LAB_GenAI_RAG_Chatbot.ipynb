{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsnCPbdkxYZd"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1 style=\"color: #FF6347;\">Self-Guided Lab: Retrieval-Augmented Generation (RAGs)</h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZp4BQAVxYZj"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ3FsdzRveTBrenMxM3VnbDMwaTJxN2NnZm50aGFibXk1NzNnY2Q0MCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/LR5ZBwZHv02lmpVoEU/giphy.gif\" alt=\"NLP Gif\" style=\"width: 300px; height: 150px; object-fit: cover; object-position: center;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gizk6HCYxYZo"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Data Storage & Retrieval</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW5UOI8ZxYZp"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">PyPDFLoader</h2>\n",
        "\n",
        "`PyPDFLoader` is a lightweight Python library designed to streamline the process of loading and parsing PDF documents for text processing tasks. It is particularly useful in Retrieval-Augmented Generation workflows where text extraction from PDFs is required.\n",
        "\n",
        "- **What Does PyPDFLoader Do?**\n",
        "  - Extracts text from PDF files, retaining formatting and layout.\n",
        "  - Simplifies the preprocessing of document-based datasets.\n",
        "  - Supports efficient and scalable loading of large PDF collections.\n",
        "\n",
        "- **Key Features:**\n",
        "  - Compatible with popular NLP libraries and frameworks.\n",
        "  - Handles multi-page PDFs and embedded images (e.g., OCR-compatible setups).\n",
        "  - Provides flexible configurations for structured text extraction.\n",
        "\n",
        "- **Use Cases:**\n",
        "  - Preparing PDF documents for retrieval-based systems in RAGs.\n",
        "  - Automating the text extraction pipeline for document analysis.\n",
        "  - Creating datasets from academic papers, technical manuals, and reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langchain langchain_community pypdf\n",
        "%pip install termcolor langchain_openai langchain-huggingface sentence-transformers chromadb langchain_chroma tiktoken openai python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6heKZkQUxYZr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/llm-env/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRS44B2XxYZs",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Loading the Documents</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cuREtJRixYZt"
      },
      "outputs": [],
      "source": [
        "# File path for the document\n",
        "\n",
        "file_path = \"/Users/turfdiddy/Desktop/Bootcamp_ds:ml/Week_7/Day_5/LAB/ai-for-everyone.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz_8SOLxxYZt"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Documents into pages</h3>\n",
        "\n",
        "The `PyPDFLoader` library allows efficient loading and splitting of PDF documents into smaller, manageable parts for NLP tasks.\n",
        "\n",
        "This functionality is particularly useful in workflows requiring granular text processing, such as Retrieval-Augmented Generation (RAG).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_b5Z_45UxYZu",
        "outputId": "a600d69f-14fe-4492-f236-97261d6ff36c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "297"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and split the document\n",
        "loader = PyPDFLoader(file_path)\n",
        "pages = loader.load_and_split()\n",
        "len(pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt50NRQaxYZv"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Pages into Chunks</h3>\n",
        "\n",
        "\n",
        "####  RecursiveCharacterTextSplitter in LangChain\n",
        "\n",
        "The `RecursiveCharacterTextSplitter` is the **recommended splitter** in LangChain when you want to break down long documents into smaller, semantically meaningful chunks — especially useful in **RAG pipelines**, where clean context chunks lead to better LLM responses.\n",
        "\n",
        "####  Parameters\n",
        "\n",
        "| Parameter       | Description                                                                 |\n",
        "|-----------------|-----------------------------------------------------------------------------|\n",
        "| `chunk_size`    | The **maximum number of characters** allowed in a chunk (e.g., `1000`).     |\n",
        "| `chunk_overlap` | The number of **overlapping characters** between consecutive chunks (e.g., `200`). This helps preserve context continuity. |\n",
        "\n",
        "####  How it works\n",
        "`RecursiveCharacterTextSplitter` attempts to split the text **intelligently**, trying the following separators in order:\n",
        "1. Paragraphs (`\"\\n\\n\"`)\n",
        "2. Lines (`\"\\n\"`)\n",
        "3. Sentences or words (`\" \"`)\n",
        "4. Individual characters (as a last resort)\n",
        "\n",
        "This makes it ideal for handling **natural language documents**, such as PDFs, articles, or long reports, without breaking sentences or paragraphs in awkward ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1096"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Alternative: CharacterTextSplitter\n",
        "\n",
        "`CharacterTextSplitter` is a simpler splitter that breaks text into chunks based **purely on character count**, without trying to preserve any natural language structure.\n",
        "\n",
        "##### Example:\n",
        "```python\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "````\n",
        "\n",
        "This method is faster and more predictable but may split text in the middle of a sentence or paragraph, which can hurt performance in downstream tasks like retrieval or QA.\n",
        "\n",
        "---\n",
        "\n",
        "#### Comparison Table\n",
        "\n",
        "| Feature                        | RecursiveCharacterTextSplitter | CharacterTextSplitter     |\n",
        "| ------------------------------ | ------------------------------ | ------------------------- |\n",
        "| Structure-aware splitting      |  Yes                          |  No                      |\n",
        "| Preserves sentence/paragraphs  |  Yes                          |  No                      |\n",
        "| Risk of splitting mid-sentence |  Minimal                     |  High                   |\n",
        "| Ideal for RAG/document QA      |  Highly recommended           |  Only if structured text |\n",
        "| Performance speed              |  Slightly slower             |  Faster                  |\n",
        "\n",
        "---\n",
        "\n",
        "#### Recommendation\n",
        "\n",
        "Use `RecursiveCharacterTextSplitter` for most real-world document processing tasks, especially when building RAG pipelines or working with structured natural language content like PDFs or articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Choosing Chunk Size in RAG\n",
        "\n",
        "### Best Practices for Chunk Size in RAG\n",
        "\n",
        "| Factor                      | Recommendation                                                                                                                                                                                          |\n",
        "| ---------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **LLM context limit**       | Choose a chunk size that lets you retrieve multiple chunks **without exceeding the model’s token limit**. For example, GPT-4o supports 128k tokens, but with GPT-3.5 (16k) or GPT-4 (32k), keep it modest. |\n",
        "| **Chunk size (in characters)** | Typically: **500–1,000 characters** per chunk → ~75–200 tokens. This fits well for retrieval + prompt without context overflow.                                                                           |\n",
        "| **Chunk size (in tokens)**  | If using token-based splitter (e.g. `TokenTextSplitter`): aim for **100–300 tokens** per chunk.                                                                                                            |\n",
        "| **Chunk overlap**           | Use **overlap of 10–30%** (e.g., 100–300 characters or ~50 tokens) to preserve context across chunk boundaries and avoid cutting off important ideas mid-sentence.                                        |\n",
        "| **Document structure**      | Use **`RecursiveCharacterTextSplitter`** to preserve semantic boundaries (paragraphs, sentences) instead of arbitrary cuts.                                                                                |\n",
        "| **Task type**               | For **question answering**, smaller chunks (~500–800 chars) reduce noise.<br>For **summarization**, slightly larger chunks (~1000–1500) are OK.                                                          |\n",
        "| **Embedding model**         | Some models (e.g., `text-embedding-3-large`) can handle long input. But still, smaller chunks give **finer-grained retrieval**, which improves relevance.                                                  |\n",
        "| **Query type**              | If users ask **very specific questions**, small focused chunks are better. For broader queries, bigger chunks might help.                                                                                  |\n",
        "\n",
        "\n",
        "### Rule of Thumb\n",
        "\n",
        "| Use Case                 | Chunk Size      | Overlap |\n",
        "| ------------------------| --------------- | ------- |\n",
        "| Factual Q&A              | 500–800 chars   | 100–200 |\n",
        "| Summarization            | 1000–1500 chars | 200–300 |\n",
        "| Technical documents      | 400–700 chars   | 100–200 |\n",
        "| Long reports/books       | 800–1200 chars  | 200–300 |\n",
        "| Small LLMs (≤16k tokens) | ≤800 chars      | 100–200 |\n",
        "\n",
        "\n",
        "### Avoid\n",
        "\n",
        "- Chunks >2000 characters: risks context overflow.\n",
        "- No overlap: may lose key information between chunks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg15RjVPxYZw"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Embeddings</h2>\n",
        "\n",
        "Embeddings transform text into dense vector representations, capturing semantic meaning and contextual relationships. They are essential for efficient document retrieval and similarity analysis.\n",
        "\n",
        "- **What are OpenAI Embeddings?**\n",
        "  - Pre-trained embeddings like `text-embedding-3-large` generate high-quality vector representations for text.\n",
        "  - Encapsulate semantic relationships in the text, enabling robust NLP applications.\n",
        "\n",
        "- **Key Features of `text-embedding-3-large`:**\n",
        "  - Large-scale embedding model optimized for accuracy and versatility.\n",
        "  - Handles diverse NLP tasks, including retrieval, classification, and clustering.\n",
        "  - Ideal for applications with high-performance requirements.\n",
        "\n",
        "- **Benefits:**\n",
        "  - Reduces the need for extensive custom training.\n",
        "  - Provides state-of-the-art performance in retrieval-augmented systems.\n",
        "  - Compatible with RAGs to create powerful context-aware models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "L0xDxElwxYZw"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_WRIo3_0xYZx",
        "outputId": "78bfbbf3-9d25-4e31-bdbc-3e932e6bbfec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MNZfTng5xYZz",
        "outputId": "db1a7c85-ef9f-447e-92cd-9d097e959847"
      },
      "outputs": [],
      "source": [
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsSA7RKvxYZz"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChromaDB</h2>\n",
        "\n",
        "ChromaDB is a versatile vector database designed for efficiently storing and retrieving embeddings. It integrates seamlessly with embedding models to enable high-performance similarity search and context-based retrieval.\n",
        "\n",
        "### Workflow Overview:\n",
        "- **Step 1:** Generate embeddings using a pre-trained model (e.g., OpenAI's `text-embedding-3-large`).\n",
        "- **Step 2:** Store the embeddings in ChromaDB for efficient retrieval and similarity calculations.\n",
        "- **Step 3:** Use the stored embeddings to perform searches, matching, or context-based retrieval.\n",
        "\n",
        "### Key Features of ChromaDB:\n",
        "- **Scalability:** Handles large-scale datasets with optimized indexing and search capabilities.\n",
        "- **Speed:** Provides fast and accurate retrieval of embeddings for real-time applications.\n",
        "- **Integration:** Supports integration with popular frameworks and libraries for embedding generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "brKe6wUgxYZ0"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VkjHR-RkxYZ0",
        "outputId": "bc11bda9-f283-457a-f584-5a06b95c4dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB created with document embeddings.\n"
          ]
        }
      ],
      "source": [
        "db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db_LAB\")\n",
        "print(\"ChromaDB created with document embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27OdN1IVxYZ1"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Retrieving Documents</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice1: Write a user question that someone might ask about your book’s topic or content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XiLv-TfrxYZ1"
      },
      "outputs": [],
      "source": [
        "user_question = \"Is AI going to replace humans one day?\" # User question\n",
        "retrieved_docs = db.similarity_search(user_question, k=10) # k is the number of documents to retrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qgWsh50JxYZ1",
        "outputId": "c8640c5d-5955-471f-fdd2-37096f5f68c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1:\n",
            "will need to coexist with machines. Jobs traditionally done by \n",
            "humans will be shifted towards AI systems. Artificial intelligence is already able \n",
            "to translate languages, diagnose illnesses, assist in retail (Kaplan 2020c), and  \n",
            "the like – in several cases, better than the human workforce. Human jobs might \n",
            "be created in the future that are unimaginable now, similar to the fact that \n",
            "nobody really predicted the job of mobile app designers just a few years ago.\n",
            "In this world, AI would rather be augmenting and complementing – rather \n",
            "than replacing – humans in their work. In the pessimistic case, i.e., massive \n",
            "unemployment, ideas such as universal basic income are already being dis -\n",
            "cussed. Fundamental philosophical questions would need to be answered sur-\n",
            "rounding life for humans when most of our work is done by AI systems. In \n",
            "any case, the State will certainly have to come up with a set of rules govern -\n",
            "Document 2:\n",
            "mple, the individual would not need to \n",
            "prepare for work anymore, as this could be done entirely by the ASI-powered \n",
            "machine or robot (Kaplan and Haenlein 2019). For a detailed discussion on the \n",
            "evolution of AI systems, we refer to Huang and Rust (2018).\n",
            "Artificial Intelligence: Machines and Humans\n",
            "In the future, artificial intelligence will raise several challenges, and humans \n",
            "will have to learn to coexist with machines and robots. Pushed by the global \n",
            "COVID-19 health crisis, it is clear that AI will deeply impact societies around \n",
            "the world (Kaplan 2021). We will discuss some of these questions, looking at \n",
            "challenges in terms of algorithms and individual organisations; the employ -\n",
            "ment market; and last but not least, democracy and human freedom potentially \n",
            "at stake due to advances in AI.\n",
            "About Algorithms and Organisations\n",
            "When machines and humans coexist, it is important that both do what they are\n",
            "Document 3:\n",
            "oyees with AI, as this would not be a long-\n",
            "term strategy. Looking at 1,500 corporations, they identified the best improve-\n",
            "ments in performance when machines and human beings work together, and \n",
            "concluded: ‘Through such collaborative intelligence, humans and AI actively \n",
            "enhance each other’s complementary strengths: the leadership, teamwork, cre-\n",
            "ativity, and social skills of the former, and the speed, scalability, and quantita -\n",
            "tive capabilities of the latter’ (Wilson and Daugherty 2018).\n",
            "However, with advances in artificial intelligence, machines improve, and \n",
            "might indeed replace humans in their jobs. It is uncertain that enough new jobs \n",
            "at the right skill levels will evolve for everybody, similar to previous shifts in\n"
          ]
        }
      ],
      "source": [
        "# Display top results\n",
        "for i, doc in enumerate(retrieved_docs[:3]): # Display top 3 results\n",
        "    print(f\"Document {i+1}:\\n{doc.page_content[36:1000]}\") # Display content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuGK8gL6xYZ1"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Preparing Content for GenAI</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2iB3lZqHxYZ2"
      },
      "outputs": [],
      "source": [
        "def _get_document_prompt(docs):\n",
        "    prompt = \"\\n\"\n",
        "    for doc in docs:\n",
        "        prompt += \"\\nContent:\\n\"\n",
        "        prompt += doc.page_content + \"\\n\\n\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2okzmuADxYZ2",
        "outputId": "0aa6cdca-188d-40e0-f5b4-8888d3549ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context formatted for GPT model.\n"
          ]
        }
      ],
      "source": [
        "# Generate a formatted context from the retrieved documents\n",
        "formatted_context = _get_document_prompt(retrieved_docs)\n",
        "print(\"Context formatted for GPT model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzIczQNTxYZ2"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChatBot Architecture</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice2: Write a prompt that is relevant and tailored to the content and style of your book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tqxVh9s3xYZ3",
        "outputId": "97cca95d-4ab3-44d8-a76c-5713aad387d8"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "\n",
        "You are a knowledgeable and factual chatbot designed to assist with technical questions about AI for everybody. Your goal is to give an in depth answer on questions asked relating to AI. The phrasing of your answers must be such that it is easily understandable by both technical and non technical people who have no knowledge in the field. Your answers must be based exclusively on provided content from technical books provided. \n",
        "\n",
        "The user has asked: \"{user_question}\"\n",
        "\n",
        "Here is the relevant content from the technical books:\n",
        "'''\n",
        "{formatted_context}\n",
        "'''\n",
        "\n",
        "Only use the content in the context section to answer. If the answer cannot be found, explicitly state: \"The provided context does not contain this information.\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0mjkQJ_ZxYZ3"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice3: Tune parameters like temperature, and penalties to control how creative, focused, or varied the model's responses are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ylypRWRlxYZ4"
      },
      "outputs": [],
      "source": [
        "# Set up GPT client and parameters\n",
        "client = openai.OpenAI()\n",
        "model_params = {\n",
        "    'model': 'gpt-4o',\n",
        "    'temperature': 0.9,  # Increase creativity\n",
        "    'max_tokens': 4000,  # Allow for longer responses\n",
        "    'top_p': 0.9,        # Use nucleus sampling\n",
        "    'frequency_penalty': 0.5,  # Reduce repetition\n",
        "    'presence_penalty': 0.7   # Encourage new topics\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8e942xDxYZ4"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Response</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4eXZO4pIxYZ4"
      },
      "outputs": [],
      "source": [
        "messages = [{'role': 'user', 'content': prompt}]\n",
        "completion = client.chat.completions.create(messages=messages, **model_params, timeout=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wLPAcchBxYZ5",
        "outputId": "976c7800-16ed-41fe-c4cf-58f60d3230d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The provided context suggests that while artificial intelligence (AI) will significantly impact society and the job market, it is more likely to augment and complement human work rather than completely replace humans. AI systems are already performing tasks like language translation and medical diagnosis, sometimes even better than humans. However, the emphasis seems to be on collaboration between humans and AI, where each enhances the other's strengths.\n",
            "\n",
            "In scenarios where AI could potentially lead to unemployment, discussions around solutions like universal basic income are already taking place. The context also highlights different perspectives: some view AI as a threat, possibly leading to massive job loss or even posing existential risks, while others see it as an opportunity for enhancing human capabilities.\n",
            "\n",
            "In conclusion, while there are concerns about AI replacing human jobs or having other negative impacts, the prevailing view in the provided content is that AI will mostly serve to enhance human work rather than entirely replace it.\n"
          ]
        }
      ],
      "source": [
        "answer = completion.choices[0].message.content\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXVNXPwLxYaT"
      },
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png\" alt=\"NLP Gif\" style=\"width: 500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldybhlqKxYaT"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Cosine Similarity</h2>\n",
        "\n",
        "**Cosine similarity** is a metric used to measure the alignment or similarity between two vectors, calculated as the cosine of the angle between them. It is the **most common metric used in RAG pipelines** for vector retrieval.. It provides a scale from -1 to 1:\n",
        "\n",
        "- **-1**: Vectors are completely opposite.\n",
        "- **0**: Vectors are orthogonal (uncorrelated or unrelated).\n",
        "- **1**: Vectors are identical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1I1TNhxYaT"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg\" alt=\"NLP Gif\" style=\"width: 700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoEMdNgQxYaU"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Keyword Highlighting</h2>\n",
        "\n",
        "Highlighting important keywords helps users quickly understand the relevance of the retrieved text to their query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nCXL9Cz1xYaV"
      },
      "outputs": [],
      "source": [
        "from termcolor import colored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwDyofY0xYaV"
      },
      "source": [
        "The `highlight_keywords` function is designed to highlight specific keywords within a given text. It replaces each keyword in the text with a highlighted version using the `colored` function from the `termcolor` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9y3E0YWExYaV"
      },
      "outputs": [],
      "source": [
        "def highlight_keywords(text, keywords):\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, colored(keyword, 'green', attrs=['bold']))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice4: add your keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "i7SkWPpnxYaW",
        "outputId": "28e82563-edba-4b41-acad-ec27e5ba134f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snippet 1:\n",
            "22 AI for Everyone?\n",
            "Clearly, \u001b[1m\u001b[32mhumans\u001b[0m will need to coexist with \u001b[1m\u001b[32mmachines\u001b[0m. Jobs traditionally done by \n",
            "\u001b[1m\u001b[32mhumans\u001b[0m will be shifted towards AI systems. Artificial intelligence is already able \n",
            "to translate lan\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "query_keywords = [\"takeover\", \"humans\",\"machines\"] # add your keywords\n",
        "for i, doc in enumerate(retrieved_docs[:1]):\n",
        "    snippet = doc.page_content[:200]\n",
        "    highlighted = highlight_keywords(snippet, query_keywords)\n",
        "    print(f\"Snippet {i+1}:\\n{highlighted}\\n{'-'*80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhV_Jf_LxYaX"
      },
      "source": [
        "1. `query_keywords` is a list of keywords to be highlighted.\n",
        "2. The loop iterates over the first document in retrieved_docs.\n",
        "3. For each document, a snippet of the first 200 characters is extracted.\n",
        "4. The highlight_keywords function is called to highlight the keywords in the snippet.\n",
        "5. The highlighted snippet is printed along with a separator line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBRKysAvxYaX"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Bonus</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj25lCybxYaX"
      },
      "source": [
        "**Try loading one of your own PDF books and go through the steps again to explore how the pipeline works with your content**:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
